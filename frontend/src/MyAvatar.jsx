/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
*/

import React, { useEffect, useRef, useState, useMemo } from 'react'
import { useGraph, useFrame } from '@react-three/fiber'
import { useGLTF } from '@react-three/drei'
import { SkeletonUtils } from 'three-stdlib'
import * as THREE from 'three'

export function Model({ modelUrl = '/models/myAvatar.glb', ...props }) {
  const { scene } = useGLTF(modelUrl)
  const clone = useMemo(() => SkeletonUtils.clone(scene), [scene])

  const { nodes, materials } = useGraph(clone)

  // -- State --
  const [speaking, setSpeaking] = useState(false)
  const [currentEmotion, setCurrentEmotion] = useState(0) // Default Neutral (0)

  // -- References --
  const meshRef = useRef(null)
  // const meshRef = useRef(null) // Removed: Replaced by meshesRef

  // Confirmed Indices for Emotions (0-5) from your list
  const emotions = {
    0: "Neutral",
    1: "Angry",
    2: "Fun",
    3: "Joy",
    4: "Sorrow",
    5: "Surprised"
  }

  // -- 1. Setup & Discovery (MULTI-MESH) --
  const meshesRef = useRef([])
  const vowelsRef = useRef([])

  useEffect(() => {
    const foundMeshes = []
    console.log("Scanning new model for ALL valid meshes...")

    Object.values(nodes).forEach((node) => {
      if (node.isMesh && node.morphTargetDictionary) {
        const keys = Object.keys(node.morphTargetDictionary)
        // Fingerprint: Look for Mouth morphs
        if (keys.includes("Fcl_MTH_A") || keys.includes("Mouth_A")) {
          console.log("VALID MESH FOUND:", node.name)
          foundMeshes.push(node)
        }
      }
    })

    if (foundMeshes.length > 0) {
      meshesRef.current = foundMeshes

      // Use the dictionary of the FIRST match to map indices 
      // (Assuming all face parts share the same morph structure, which is standard)
      const dict = foundMeshes[0].morphTargetDictionary
      const getIdx = (candidates) => {
        for (let c of candidates) if (dict[c] !== undefined) return dict[c]
        return -1
      }

      const newVowels = [
        getIdx(["Fcl_MTH_A", "Mouth_A", "A"]),
        getIdx(["Fcl_MTH_I", "Mouth_I", "I"]),
        getIdx(["Fcl_MTH_U", "Mouth_U", "U"]),
        getIdx(["Fcl_MTH_E", "Mouth_E", "E"]),
        getIdx(["Fcl_MTH_O", "Mouth_O", "O"])
      ].filter(idx => idx !== -1)

      if (newVowels.length > 0) {
        console.log("Using Vowel Indices:", newVowels)
        vowelsRef.current = newVowels
      } else {
        vowelsRef.current = [39, 40, 41, 42, 43]
      }
    } else {
      console.error("CRITICAL: No meshes found with Mouth morphs.")
    }
  }, [nodes])


  // -- 2. Animation Loop (Apply to ALL matches) --
  useFrame((state) => {
    if (meshesRef.current.length === 0) return

    const t = state.clock.elapsedTime

    // Calculate Values
    // 1. Emotion (0 or 1)
    const emotionWeights = {}
    for (let i = 0; i <= 5; i++) {
      emotionWeights[i] = (i === currentEmotion) ? 1 : 0
    }

    // 2. Speech (0 to 1)
    let open = 0
    if (speaking) {
      open = (Math.sin(t * 30) + 1) * 0.5 + Math.random() * 0.3
      open = Math.min(open, 1.0)
    }

    // Apply to ALL meshes
    meshesRef.current.forEach(mesh => {
      if (!mesh.morphTargetInfluences) return

      // A. Emotions
      for (let i = 0; i <= 5; i++) {
        mesh.morphTargetInfluences[i] = THREE.MathUtils.lerp(mesh.morphTargetInfluences[i], emotionWeights[i], 0.1)
      }

      // B. Lip Sync
      if (speaking) {
        vowelsRef.current.forEach(idx => {
          if (idx < mesh.morphTargetInfluences.length) {
            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(mesh.morphTargetInfluences[idx], open, 0.4)
          }
        })
      } else {
        vowelsRef.current.forEach(idx => {
          if (idx < mesh.morphTargetInfluences.length) {
            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(mesh.morphTargetInfluences[idx], 0, 0.1)
          }
        })
      }
    })
  })

  // -- 3. Scripted Demo --
  const runDemo = () => {
    // DEBUG: Log all available morphs to help user verify
    if (meshesRef.current.length > 0) { // Changed from meshRef.current
      console.log("=== FULL MORPH DICTIONARY FOR CURRENT AVATAR ===")
      console.log(Object.keys(meshesRef.current[0].morphTargetDictionary))
      console.log("================================================")
    } else {
      console.warn("No Mesh Found to Log.")
    }

    if (speaking) return
    setSpeaking(true)

    const text = "I am checking my morph targets. Please check the console log."
    const utterance = new SpeechSynthesisUtterance(text)
    utterance.rate = 0.9

    setCurrentEmotion(0); setTimeout(() => setCurrentEmotion(1), 2500)
    setTimeout(() => setCurrentEmotion(2), 4500); setTimeout(() => setCurrentEmotion(3), 6000)
    setTimeout(() => setCurrentEmotion(4), 8000); setTimeout(() => setCurrentEmotion(5), 10000)
    setTimeout(() => setCurrentEmotion(0), 13000)

    utterance.onend = () => setSpeaking(false)
    window.speechSynthesis.speak(utterance)
  }

  useEffect(() => {
    window.addEventListener('click', runDemo)
    return () => window.removeEventListener('click', runDemo)
  }, [speaking])

  return (
    <group {...props} dispose={null}>
      {/* RENDER THE CLONE DIRECTLY - ensures animation loop targets visible mesh */}
      <primitive object={clone} />
    </group>
  )
}


